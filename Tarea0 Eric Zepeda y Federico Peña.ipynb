{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2:\n",
    "\n",
    "## Job salary prediction\n",
    "Se trabajará con el problema de identificar cual es el sueldo que ofrecen los avisos, aunque no este de forma explicita.\n",
    "\n",
    "> a) Carge los datos csv de entrenamiento y cree un conjunto de validación con los últimos 10 mil datos en un dataframe de pandas. Describa los datos, apóyese de gráficos ¿Cuántos datos hay en cada conjunto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Train_rev1.csv\")\n",
    "df_train = df.iloc[:-10000]\n",
    "df_val = df.iloc[-10000:]\n",
    "rows = [line.split(\" \",1) for line in df_train['FullDescription']()]\n",
    "\n",
    "df_train.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['SalaryNormalized'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.countplot(x= 'SalaryNormalized',data=df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada dato tiene 12 parámetros, estos se describen por si mismo. Id y SalaryNormalized representan valores enteros, el resto son strings.\n",
    "El arreglo es una matriz de 234768 x 12 donde cada fila es un dato con sus parámetros, algunas filas tienen parametros vacios.\n",
    "Se trata de un problema de  valoración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> b) Extraiga los datos de cada conjunto con los que trabajará, el input  XX , los textos, y el output  yy , los salarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos que nos interesan para analizar serán aquellos que entregen información sobre el trabajo, sus características y requerimientos. La descripción del puesto, su categoria y el titulo profesional, ademas del tipo y tiempo de contrato y la compañía pueden afectar a dinero ofrecido. De igual manera la ubicación puede afectar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.FullDescription\n",
    "salary = df.SalaryNormalized\n",
    "rows = [line.split(\" \") for line in text]\n",
    "df_train = pd.DataFrame(rows, columns=['Text','Salary'])\n",
    "df_train.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> c) Realice un pre-procesamiento a los datos brutos de texto para extraer características y generar la representación de los datos de entrada al modelo $\\vec{x}$. Comente sobre lo realizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re, time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer, word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "def word_extractor1(text):\n",
    "    wordlemmatizer = WordNetLemmatizer()\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ wordlemmatizer.lemmatize(word.lower())for word in word_tokenize(text.decode('utf-8', 'ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "\n",
    "print(word_extractor1('I love to eat cake'))\n",
    "print(word_extractor1(\"I love eating cake\"))\n",
    "print(word_extractor1(\"I loved eating the cake\"))\n",
    "print(word_extractor1(\"I do not love eating cake\"))\n",
    "print(word_extractor1(\"I don't love eating cake\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "> d) Intente resolver el problema enfrentándolo como regresión con el modelo de regresión lineal ordinaria en *sklearn*. ¿Qué es lo que hace *fit_intercept=True*? Evalúe la función objetivo (F.O.) utilizada y la métrica de la competencia (*mean absolute error*) en ambos conjuntos generados en el punto a). Comente lo observado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression as LR\n",
    "model = LR(fit_intercept=True, normalize=False)\n",
    "model.fit(text,salary)\n",
    "\n",
    "#measure F.O.\n",
    "y=df.array(salary)\n",
    "x=df.array(text)\n",
    "\n",
    "mean_absolute_error=MAE\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error #measure MAE\n",
    "print(\"MAE on train: \",mean_absolute_error(salary, model.predict(text))\n",
    "#print(\"MAE on validation: \",mean_absolute_error(salary, model.predict(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3:\n",
    "\n",
    "Para el desarrollo de la parte 3 se desarrollaron funciones que satisfacen lo estipulado en cada item y considerando además la incertidumbre sobre si realmente la pendiente b que estamos buscando es constante, ya que la funcion gradiente de b que podemos obtener analiticamente dependerá de los puntos a considerar a no ser que se fijen las condiciones necesarias que veremos más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def productopunto(x,b):\n",
    "    b = np.asarray(b)\n",
    "    x = np.asarray(x)\n",
    "    prod = 0\n",
    "    for i in range(b.size):\n",
    "        prod = prod + b[i]*x[i]\n",
    "    return prod;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se resumen las funciones generadas en la parte a considerando el caso de una dimension (loss_1) y una para varias dimensiones (loss_n) (parte a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_1(b,x,y):\n",
    "    #Esto fue introducido para que python reconozca los arreglos recibidos\n",
    "    b = np.asarray(b) # b es la pendiente que queremos encontrar\n",
    "    x = np.asarray(x)# x corresponde al eje x\n",
    "    y = np.asarray(y)# y corresponde al eje y\n",
    "    loss=0\n",
    "    n = y.size\n",
    "    for i in range(n):\n",
    "        loss = loss + ((b[i]*x[i] - y[i] )**2)/n# mas que nada aqui calculamos la funcion perdida.\n",
    "    return loss\n",
    "\n",
    "def loss_n(b,x,y):\n",
    "    b = np.asarray(b)\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    nx = np.shape(x)\n",
    "    loss=0\n",
    "    for i in range(nx[1]):\n",
    "        p_product = productopunto(x[i],b)#en varias variablesconsideramos bx como un funcional de R^n -> R   con n>1 \n",
    "        loss = loss +  ((p_product - y[i])**2)/nx[0]\n",
    "    return loss; \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La parte siguiente corresponde al calculo del gradiente para ser empleado en los otros ejercicios, esto sería equivalente a la parte (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_1(b,x,y):\n",
    "    grad = 0\n",
    "    b = np.asarray(b)\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    n = y.size\n",
    "    for i in range(n):\n",
    "        grad = grad + (x[i]*(b[i]*x[i] - y[i] )*2)/n # es el producto punto.\n",
    "    return grad;\n",
    "def grad_n(b,x,y):\n",
    "    grad = 0\n",
    "    b = np.asarray(b)\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    nx = np.shape(x)\n",
    "    for i in range(nx[1]):\n",
    "        p_product = productopunto(x[i],b)\n",
    "        grad = grad + (np.multiply(X[i],(p_product - Y[i] )*2*(1/nx[0])))\n",
    "    return grad;        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la parte c con la ayuda de los apuntes del curso y la bibliografia se crearon dos funciones que calculan la derivada analiticamente para 1 dimension (R->R ) y para (R^n -> R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dados dos puntos x,y, estas funciones calculan el gradiente respecto a los valores entregados a la funcion\n",
    "\n",
    "def grad_1real(x,y): # la derivada en una dimension\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    b = 0    \n",
    "    n = y.size\n",
    "    if x!=0: \n",
    "        b = (y/x) \n",
    "    else:\n",
    "        b = 0 \n",
    "    return b\n",
    "# aqui se entregan balores para x,y, pero consideraremos x como un vector de n dimensiones\n",
    "def grad_nreal(x,y):#Esto fue una prueba considerando varias dimensiones para el dominio y la pendiente no consante\n",
    "    x = np.asarray(x)#Qué sucedería con nuestra regresion si se aproxima a la pendiente más pequeña o la más grande?\n",
    "    y = np.asarray(y)#solucion: el error seguiria siendo alto\n",
    "    nx = np.shape(x)\n",
    "    b = np.zeros(nx[1]) \n",
    "    temp = np.zeros(nx[1])\n",
    "    error_temp = 8888\n",
    "    for i in range(nx[1]):\n",
    "        rest = productopunto(x[i],x[i])\n",
    "        if rest!=0: \n",
    "            b = (1/rest)*(y[i]*x[i]) \n",
    "            loss = loss_n(b,x,y)\n",
    "            if loss<error_temp:\n",
    "                temp = b\n",
    "                error_temp = loss\n",
    "            else:\n",
    "                continue\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parte C\n",
    "def grad_analitico(x,y):\n",
    "    x = np.asarray(x)#Qué sucedería con nuestra regresion si se aproxima a la pendiente más pequeña o la más grande?\n",
    "    y = np.asarray(y)#solucion: el error seguiria siendo alto\n",
    "    nx = np.shape(x)\n",
    "    b = np.zeros(nx[1])\n",
    "    x_tr =x.transpose() \n",
    "    x_trx_inv = np.linalg.inv(np.matmul(x_tr,x))\n",
    "    b = (np.matmul(x_trx_inv,x_tr)).dot(y)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se presenta el algoritmo de entrenamiento con una condición extra que consiste en añadir un número finito de iteraciones para evitar loops largos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(x,y,nn, tol):#esta funcion solo se creo en varias variables por el hecho de que en una variable es más directo\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    nx = np.shape(x)\n",
    "    b = np.zeros(nx[1])#genero un vector b de las dimensiones de x\n",
    "    dummycontrol = 0 # es una variable para evitar loop infinitos y acabar de golpe.\n",
    "    grad_f = 0\n",
    "    loss = 5000\n",
    "    while loss >  tol and dummycontrol <= 1000000000:\n",
    "        grad_f = grad_n(b,x,y)#se utilizan las tres funciones para calcular el gradiente\n",
    "        b = b - np.multiply(nn,grad_f)# emplear el algoritmo\n",
    "        loss = loss_n(b,x,y)# calcular el error asociado y esperar de buena fe que sea menor a la tolerancia aceptada\n",
    "        dummycontrol+= 1\n",
    "    return b\n",
    "    \n",
    "def SGD_it_error(x,y,nn,it):\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    nx = np.shape(x)\n",
    "    y_loss = np.zeros(1000)\n",
    "    b = np.zeros(nx[1])#genero un vector b de las dimensiones de x\n",
    "    dummycontrol = 0 # es una variable para evitar loop infinitos y acabar de golpe.\n",
    "    grad_f = 0\n",
    "    loss = 5000\n",
    "    while dummycontrol < it:\n",
    "        grad_f = grad_n(b,x,y)#se utilizan las tres funciones para calcular el gradiente\n",
    "        b = b - nn*grad_f# emplear el algoritmo\n",
    "        loss = loss_n(b,x,y)# calcular el error asociado y esperar de buena fe que sea menor a la tolerancia aceptada\n",
    "        y_loss[dummycontrol] = loss\n",
    "        dummycontrol+= 1\n",
    "    return  y_loss;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Parte D\n",
    "import matplotlib.pyplot as plt # grafico del error\n",
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt \n",
    "X_train,y_train = load_boston(return_X_y=True)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)#cada vector tiene 13 componentes y son 506 los que entrega\n",
    "X = X_train\n",
    "Y = y_train\n",
    "x_loss = np.arange(1000)#nro iteraciones\n",
    "y_loss = SGD_it_error(X,Y,0.5,1000)\n",
    "plt.plot(x_loss,y_loss)\n",
    "plt.show() \n",
    "        \n",
    "    \n",
    "#https://gluon.mxnet.io/chapter06_optimization/gd-sgd-scratch.html\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la parte F consideraremos solo los print con coeficientes de 0.1 a 0.1 sin incluir los bordes del intervalo (0,1) a partir de las funciones antes creadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con coeficiente de aprendizaje = 0.1,\n",
      "b =  [-2.83444559 -2.46408404 -3.94024081 -1.87248978 -4.34110785  4.01439056\n",
      " -1.85483104  5.00240588 -4.44824688 -5.95803226 -4.12393269  2.88457616\n",
      " -0.91099871]\n",
      "Con error =  0.2999947261157933\n",
      "Con coeficiente de aprendizaje = 0.2,\n",
      "b =  [-2.83464056 -2.46461257 -3.93942755 -1.87261068 -4.34158489  4.0153347\n",
      " -1.85561431  5.00201784 -4.44832633 -5.95840239 -4.12410126  2.88479753\n",
      " -0.91006889]\n",
      "Con error =  0.2999645715580321\n",
      "Con coeficiente de aprendizaje = 0.3,\n",
      "b =  [-2.83449517 -2.46433694 -3.939924   -1.87252553 -4.34132347  4.01488032\n",
      " -1.85526124  5.00244158 -4.44829283 -5.95819002 -4.12382812  2.8845902\n",
      " -0.91040734]\n",
      "Con error =  0.2999809607989272\n",
      "Con coeficiente de aprendizaje = 0.4,\n",
      "b =  [-2.83434974 -2.46406125 -3.9404207  -1.87244035 -4.34106202  4.01442564\n",
      " -1.85490793  5.00286551 -4.44825923 -5.9579776  -4.12355477  2.88438283\n",
      " -0.91074593]\n",
      "Con error =  0.29999736152821377\n",
      "Con coeficiente de aprendizaje = 0.5,\n",
      "b =  [-2.83505536 -2.46579535 -3.93764243 -1.87287038 -4.34264611  4.01746768\n",
      " -1.85739655  5.00125857 -4.44850847 -5.95922113 -4.12438785  2.88524738\n",
      " -0.90791468]\n",
      "Con error =  0.2998974423522696\n",
      "Con coeficiente de aprendizaje = 0.6,\n",
      "b =  [-2.83508021 -2.46592173 -3.93748368 -1.8728883  -4.34275386  4.01771291\n",
      " -1.85761235  5.00127608 -4.44853155 -5.95929999 -4.12433613  2.88525443\n",
      " -0.90761887]\n",
      "Con error =  0.2998905626999678\n",
      "Con coeficiente de aprendizaje = 0.7,\n",
      "b =  [-2.83510507 -2.46604813 -3.93732489 -1.87290621 -4.34286162  4.01795821\n",
      " -1.85782829  5.0012936  -4.44855464 -5.95937885 -4.12428444  2.88526149\n",
      " -0.90732296]\n",
      "Con error =  0.29988368217265926\n",
      "Con coeficiente de aprendizaje = 0.8,\n",
      "b =  [-2.83512994 -2.46617455 -3.93716605 -1.87292414 -4.34296939  4.01820358\n",
      " -1.85804438  5.00131112 -4.44857773 -5.95945772 -4.12423279  2.88526855\n",
      " -0.90702696]\n",
      "Con error =  0.29987680077022133\n",
      "Con coeficiente de aprendizaje = 0.9,\n",
      "b =  [-2.83566524 -2.46750459 -3.93504362 -1.87325103 -4.34418253  4.02054411\n",
      " -1.85996357  5.00010844 -4.44877069 -5.96040884 -4.12484773  2.88591882\n",
      " -0.90483385]\n",
      "Con error =  0.2998003404534236\n"
     ]
    }
   ],
   "source": [
    "#Parte F\n",
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt \n",
    "X_train,y_train = load_boston(return_X_y=True)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)#cada vector tiene 13 componentes y son 506 los que entrega\n",
    "X = X_train\n",
    "Y = y_train\n",
    "count = 0\n",
    "tol = 0.3\n",
    "b1 = SGD(X,Y, 0.1, tol)\n",
    "print(\"Con coeficiente de aprendizaje = 0.1,\")\n",
    "print(\"b = \", b1)\n",
    "print(\"Con error = \", loss_n(b1,X,Y))\n",
    "b2 = SGD(X,Y, 0.2, tol)\n",
    "print(\"Con coeficiente de aprendizaje = 0.2,\")\n",
    "print(\"b = \",b2)\n",
    "print(\"Con error = \", loss_n(b2,X,Y))\n",
    "b3 = SGD(X,Y, 0.3, tol)\n",
    "print(\"Con coeficiente de aprendizaje = 0.3,\")\n",
    "print(\"b = \", b3)\n",
    "print(\"Con error = \", loss_n(b3,X,Y))\n",
    "b4 = SGD(X,Y, 0.4, tol)\n",
    "print(\"Con coeficiente de aprendizaje = 0.4,\")\n",
    "print(\"b = \", b4)\n",
    "print(\"Con error = \", loss_n(b4,X,Y))\n",
    "b5 = SGD(X,Y, 0.5, tol)\n",
    "print(\"Con coeficiente de aprendizaje = 0.5,\")\n",
    "print(\"b = \", b5)\n",
    "print(\"Con error = \", loss_n(b5,X,Y))\n",
    "b6 = SGD(X,Y, 0.6, tol)\n",
    "print(\"Con coeficiente de aprendizaje = 0.6,\")\n",
    "print(\"b = \", b6)\n",
    "print(\"Con error = \", loss_n(b6,X,Y))\n",
    "b7 = SGD(X,Y, 0.7, tol)\n",
    "print(\"Con coeficiente de aprendizaje = 0.7,\")\n",
    "print(\"b = \", b7)\n",
    "print(\"Con error = \", loss_n(b7,X,Y))\n",
    "b8 = SGD(X,Y, 0.8, tol)\n",
    "print(\"Con coeficiente de aprendizaje = 0.8,\")\n",
    "print(\"b = \", b8)\n",
    "print(\"Con error = \", loss_n(b8,X,Y))\n",
    "b9 = SGD(X,Y, 0.9, tol)\n",
    "print(\"Con coeficiente de aprendizaje = 0.9,\")\n",
    "print(\"b = \", b9)\n",
    "print(\"Con error = \", loss_n(b9,X,Y))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note que para los datos que tenemos a pesar de ser XXX, se consideraron todos como datos de entrenamiento y por el hecho de que el conjunto de los números reales es no numerable, falta información en el entrenamiento para tener una pendiente b más precisa y además que a medida que se incrementa el coeficiente, los valores obtenidos aumentan hasta cierto punto donde empiezan a decrecer, lo que nos da pistas acerca del minimo global que claramente es donde el algoritmo SGD que con toques del teorema del punto fijo de Banach sobre las pendientes hasta converger a cero donde obtenemos el x que estamos buscando, el cual es XXX y se llega más cerca cuando el coeficiente de aprendisaje XXX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "por lo que entendi se utiliza el teorema del punto fijo de banach para dar existencia a un punto fijo, pero esto se aplica sobre las pendientes de la función loss sobre los datos que estamos tomando, por lo que cada vez el gradiente se acerca a cero y converge al minimo que estamos buscando para la regresión lineal. Por otra parte, cabe destacar que al considerar una regresión lineal para los datos en cuestión, el error es alto por lo dispersos que están los datos y por el hecho de que el gradiente calculado analiticamente es una función que va desde Grad: R^13 -> L(R^13,R), el gradiente nos lleva desde el dominio al espacio de los funcionales continuos y acotados, por lo que el gradiente no considera necesariamente a la pendiente como un vector constante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.92041113  1.08098058  0.14296712  0.68220346 -2.06009246  2.67064141\n",
      "  0.02112063 -3.10444805  2.65878654 -2.07589814 -2.06215593  0.85664044\n",
      " -3.74867982]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "b = grad_analitico(X,Y)\n",
    "\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte F considerando una porcion aleatoria de datos \n",
    "#index = np.zeros(250)\n",
    "#dummycount = 0\n",
    "#dummyindex = 0\n",
    "#tol = 0.3\n",
    "#train_x = np.zeros((250,13))\n",
    "#train_y = np.zeros(250)\n",
    "#for i in range (250):\n",
    "#    train_x[dummyindex] = X[dummycount]\n",
    "#    train_y[dummyindex] = Y[dummycount]\n",
    "#    dummycount+=2\n",
    "#    dummyindex+=1\n",
    "#b =SGD(train_x,train_y,0.01,tol)\n",
    "#print(\"error = \",loss_n(b,X,Y))\n",
    "#print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
